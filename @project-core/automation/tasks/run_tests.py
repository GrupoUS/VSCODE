#!/usr/bin/env python3
"""
üß™ RUN TESTS - VIBECODE SYSTEM V4.0

Consolida√ß√£o de todos os scripts de teste e valida√ß√£o:
- finaltest.ps1, finaltest-v4.ps1, enhanced-finaltest-v3.1.ps1
- finaltest-enhanced.ps1, finaltest-unified-memory-system.ps1
- finaltest-backup-protection.ps1, simple-validation-test.ps1
- validate-system-clean.ps1

Autor: GRUPO US - VIBECODE SYSTEM
Data: 2025-01-27
"""

import os
import sys
import json
import argparse
import subprocess
import shutil
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import logging

# Configura√ß√£o de logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class TestRunner:
    """Executor consolidado de testes e valida√ß√µes VIBECODE V4.0."""

    def __init__(self, project_root: str = None):
        self.project_root = Path(project_root) if project_root else Path.cwd()
        self.project_core = self.project_root / "@project-core"
        self.memory_path = self.project_core / "memory"
        self.reports_path = self.project_core / "reports"
        self.reports_path.mkdir(exist_ok=True)
        
        # Configura√ß√µes de teste
        self.critical_files = [
            "@project-core/memory/master_rule.md",
            "@project-core/memory/self_correction_log.md",
            "@project-core/memory/global-standards.md",
            "@project-core/rules/00-vibecode-system-v4-master.md"
        ]
        
        self.critical_directories = [
            "@project-core/memory",
            "@project-core/rules",
            "@project-core/automation",
            "@project-core/configs"
        ]

    def run_basic_tests(self, backup_protection: bool = False) -> Dict[str, bool]:
        """
        Executa testes b√°sicos do sistema.
        Equivalente a: simple-validation-test.ps1, validate-system-clean.ps1
        """
        logger.info("üß™ Executando testes b√°sicos...")
        
        results = {}
        
        # Teste 1: Verificar arquivos cr√≠ticos
        results['critical_files'] = self._test_critical_files()
        
        # Teste 2: Verificar estrutura de diret√≥rios
        results['directory_structure'] = self._test_directory_structure()
        
        # Teste 3: Verificar integridade do memory bank
        results['memory_bank'] = self._test_memory_bank()
        
        # Teste 4: Verificar status Git
        results['git_status'] = self._test_git_status()
        
        # Teste 5: Prote√ß√£o de backup (se solicitado)
        if backup_protection:
            results['backup_protection'] = self._test_backup_protection()
        
        return results

    def run_enhanced_tests(self, memory_validation: bool = True) -> Dict[str, bool]:
        """
        Executa testes aprimorados com valida√ß√£o de mem√≥ria.
        Equivalente a: finaltest-enhanced.ps1, finaltest-unified-memory-system.ps1
        """
        logger.info("üöÄ Executando testes aprimorados...")
        
        results = self.run_basic_tests(backup_protection=True)
        
        # Testes adicionais aprimorados
        results['automation_scripts'] = self._test_automation_scripts()
        results['mcp_integration'] = self._test_mcp_integration()
        results['performance_metrics'] = self._test_performance_metrics()
        
        if memory_validation:
            results['memory_validation'] = self._test_memory_validation()
            results['unified_memory_system'] = self._test_unified_memory_system()
        
        return results

    def run_comprehensive_tests(self) -> Dict[str, bool]:
        """
        Executa suite completa de testes.
        Equivalente a: finaltest-v4.ps1, enhanced-finaltest-v3.1.ps1
        """
        logger.info("üéØ Executando testes abrangentes...")
        
        results = self.run_enhanced_tests(memory_validation=True)
        
        # Testes abrangentes adicionais
        results['dependency_check'] = self._test_dependencies()
        results['security_validation'] = self._test_security()
        results['workflow_integration'] = self._test_workflow_integration()
        results['cross_platform_compatibility'] = self._test_cross_platform()
        
        return results

    def _test_critical_files(self) -> bool:
        """Testa exist√™ncia e integridade de arquivos cr√≠ticos."""
        logger.info("üìÅ Testando arquivos cr√≠ticos...")
        
        missing_files = []
        for file_path in self.critical_files:
            full_path = self.project_root / file_path
            if not full_path.exists():
                missing_files.append(file_path)
                logger.error(f"‚ùå Arquivo cr√≠tico ausente: {file_path}")
            elif full_path.stat().st_size == 0:
                missing_files.append(file_path)
                logger.error(f"‚ùå Arquivo cr√≠tico vazio: {file_path}")
            else:
                logger.info(f"‚úÖ Arquivo cr√≠tico OK: {file_path}")
        
        return len(missing_files) == 0

    def _test_directory_structure(self) -> bool:
        """Testa estrutura de diret√≥rios obrigat√≥ria."""
        logger.info("üìÇ Testando estrutura de diret√≥rios...")
        
        missing_dirs = []
        for dir_path in self.critical_directories:
            full_path = self.project_root / dir_path
            if not full_path.exists():
                missing_dirs.append(dir_path)
                logger.error(f"‚ùå Diret√≥rio cr√≠tico ausente: {dir_path}")
            else:
                logger.info(f"‚úÖ Diret√≥rio cr√≠tico OK: {dir_path}")
        
        return len(missing_dirs) == 0

    def _test_memory_bank(self) -> bool:
        """Testa integridade do memory bank."""
        logger.info("üß† Testando memory bank...")
        
        memory_files = ['master_rule.md', 'self_correction_log.md', 'global-standards.md']
        issues = []
        
        for file in memory_files:
            path = self.memory_path / file
            if not path.exists():
                issues.append(f"Memory file missing: {file}")
            elif path.stat().st_size < 100:  # Arquivo muito pequeno
                issues.append(f"Memory file too small: {file}")
            else:
                logger.info(f"‚úÖ Memory file OK: {file}")
        
        if issues:
            for issue in issues:
                logger.error(f"‚ùå {issue}")
            return False
        
        return True

    def _test_git_status(self) -> bool:
        """Testa status do reposit√≥rio Git."""
        logger.info("üìä Testando status Git...")
        
        try:
            # Verificar se √© um reposit√≥rio Git
            result = subprocess.run(['git', 'status', '--porcelain'], 
                                  capture_output=True, text=True, cwd=self.project_root)
            
            if result.returncode != 0:
                logger.error("‚ùå N√£o √© um reposit√≥rio Git v√°lido")
                return False
            
            # Verificar arquivos n√£o commitados
            if result.stdout.strip():
                logger.warning("‚ö†Ô∏è Existem arquivos n√£o commitados")
                return True  # N√£o √© erro cr√≠tico
            
            logger.info("‚úÖ Reposit√≥rio Git limpo")
            return True
            
        except FileNotFoundError:
            logger.error("‚ùå Git n√£o encontrado no sistema")
            return False

    def _test_backup_protection(self) -> bool:
        """Testa sistema de prote√ß√£o de backup."""
        logger.info("üõ°Ô∏è Testando prote√ß√£o de backup...")
        
        backup_dir = self.project_core / "backups"
        if not backup_dir.exists():
            logger.warning("‚ö†Ô∏è Diret√≥rio de backup n√£o existe")
            return True  # N√£o √© erro cr√≠tico
        
        # Verificar se h√° backups recentes (√∫ltimos 7 dias)
        recent_backups = []
        cutoff_time = datetime.now().timestamp() - (7 * 24 * 60 * 60)
        
        for backup_item in backup_dir.iterdir():
            if backup_item.stat().st_mtime > cutoff_time:
                recent_backups.append(backup_item.name)
        
        if recent_backups:
            logger.info(f"‚úÖ {len(recent_backups)} backups recentes encontrados")
            return True
        else:
            logger.warning("‚ö†Ô∏è Nenhum backup recente encontrado")
            return True  # N√£o √© erro cr√≠tico

    def _test_automation_scripts(self) -> bool:
        """Testa scripts de automa√ß√£o."""
        logger.info("‚öôÔ∏è Testando scripts de automa√ß√£o...")
        
        automation_dir = self.project_core / "automation"
        python_scripts = list(automation_dir.glob("*.py"))
        
        if len(python_scripts) < 5:
            logger.error("‚ùå Poucos scripts Python encontrados")
            return False
        
        logger.info(f"‚úÖ {len(python_scripts)} scripts Python encontrados")
        return True

    def _test_mcp_integration(self) -> bool:
        """Testa integra√ß√£o MCP."""
        logger.info("üîå Testando integra√ß√£o MCP...")
        
        mcp_config = self.project_core / "configs" / "mcp-servers.json"
        if not mcp_config.exists():
            logger.error("‚ùå Configura√ß√£o MCP n√£o encontrada")
            return False
        
        try:
            with open(mcp_config, 'r') as f:
                config = json.load(f)
            
            if 'mcpServers' not in config:
                logger.error("‚ùå Configura√ß√£o MCP inv√°lida")
                return False
            
            logger.info(f"‚úÖ {len(config['mcpServers'])} servidores MCP configurados")
            return True
            
        except json.JSONDecodeError:
            logger.error("‚ùå Configura√ß√£o MCP com JSON inv√°lido")
            return False

    def _test_performance_metrics(self) -> bool:
        """Testa m√©tricas de performance."""
        logger.info("üìà Testando m√©tricas de performance...")
        
        # Teste simples de performance do sistema
        start_time = datetime.now()
        
        # Simular opera√ß√£o de I/O
        test_file = self.project_core / "test_performance.tmp"
        try:
            test_file.write_text("test" * 1000)
            content = test_file.read_text()
            test_file.unlink()
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            if duration > 1.0:  # Mais de 1 segundo √© lento
                logger.warning(f"‚ö†Ô∏è Performance lenta: {duration:.2f}s")
                return True  # N√£o √© erro cr√≠tico
            
            logger.info(f"‚úÖ Performance OK: {duration:.2f}s")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erro no teste de performance: {e}")
            return False

    def _test_memory_validation(self) -> bool:
        """Testa valida√ß√£o espec√≠fica de mem√≥ria."""
        logger.info("üß† Testando valida√ß√£o de mem√≥ria...")
        
        # Verificar se arquivos de mem√≥ria t√™m conte√∫do v√°lido
        master_rule = self.memory_path / "master_rule.md"
        if master_rule.exists():
            content = master_rule.read_text()
            if "VIBECODE SYSTEM" in content and len(content) > 1000:
                logger.info("‚úÖ Master rule v√°lido")
                return True
        
        logger.error("‚ùå Master rule inv√°lido ou ausente")
        return False

    def _test_unified_memory_system(self) -> bool:
        """Testa sistema unificado de mem√≥ria."""
        logger.info("üîó Testando sistema unificado de mem√≥ria...")
        
        # Verificar arquivos do sistema de mem√≥ria
        memory_files = [
            "master_rule.md",
            "self_correction_log.md", 
            "global-standards.md"
        ]
        
        valid_files = 0
        for file in memory_files:
            path = self.memory_path / file
            if path.exists() and path.stat().st_size > 100:
                valid_files += 1
        
        if valid_files == len(memory_files):
            logger.info("‚úÖ Sistema unificado de mem√≥ria OK")
            return True
        else:
            logger.error(f"‚ùå Sistema de mem√≥ria incompleto: {valid_files}/{len(memory_files)}")
            return False

    def _test_dependencies(self) -> bool:
        """Testa depend√™ncias do sistema."""
        logger.info("üì¶ Testando depend√™ncias...")
        
        requirements_file = self.project_core / "requirements.txt"
        if not requirements_file.exists():
            logger.error("‚ùå requirements.txt n√£o encontrado")
            return False
        
        logger.info("‚úÖ requirements.txt encontrado")
        return True

    def _test_security(self) -> bool:
        """Testa valida√ß√µes de seguran√ßa."""
        logger.info("üîí Testando seguran√ßa...")
        
        # Verificar se n√£o h√° secrets expostos
        gitignore = self.project_root / ".gitignore"
        if gitignore.exists():
            content = gitignore.read_text()
            if ".env" in content and "*.key" in content:
                logger.info("‚úÖ .gitignore configurado para seguran√ßa")
                return True
        
        logger.warning("‚ö†Ô∏è .gitignore pode n√£o estar configurado adequadamente")
        return True  # N√£o √© erro cr√≠tico

    def _test_workflow_integration(self) -> bool:
        """Testa integra√ß√£o de workflows."""
        logger.info("üîÑ Testando integra√ß√£o de workflows...")
        
        workflows_dir = self.project_root / ".github" / "workflows"
        if workflows_dir.exists() and list(workflows_dir.glob("*.yml")):
            logger.info("‚úÖ Workflows GitHub Actions encontrados")
            return True
        
        logger.warning("‚ö†Ô∏è Workflows GitHub Actions n√£o encontrados")
        return True  # N√£o √© erro cr√≠tico

    def _test_cross_platform(self) -> bool:
        """Testa compatibilidade cross-platform."""
        logger.info("üåê Testando compatibilidade cross-platform...")
        
        # Teste simples de compatibilidade
        import platform
        system = platform.system()
        
        logger.info(f"‚úÖ Sistema detectado: {system}")
        return True

    def generate_report(self, results: Dict[str, bool], level: str) -> str:
        """Gera relat√≥rio de resultados."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = self.reports_path / f"test_report_{level}_{timestamp}.json"
        
        # Calcular estat√≠sticas
        total_tests = len(results)
        passed_tests = sum(results.values())
        success_rate = (passed_tests / total_tests) * 100 if total_tests > 0 else 0
        
        report_data = {
            "timestamp": timestamp,
            "level": level,
            "total_tests": total_tests,
            "passed_tests": passed_tests,
            "failed_tests": total_tests - passed_tests,
            "success_rate": round(success_rate, 2),
            "results": results,
            "summary": f"{passed_tests}/{total_tests} testes passaram ({success_rate:.1f}%)"
        }
        
        # Salvar relat√≥rio
        with open(report_file, 'w') as f:
            json.dump(report_data, f, indent=2)
        
        # Log do resultado
        if success_rate >= 90:
            logger.info(f"‚úÖ SUCESSO: {report_data['summary']}")
        elif success_rate >= 70:
            logger.warning(f"‚ö†Ô∏è PARCIAL: {report_data['summary']}")
        else:
            logger.error(f"‚ùå FALHA: {report_data['summary']}")
        
        logger.info(f"üìä Relat√≥rio salvo: {report_file}")
        return str(report_file)

def main():
    """Fun√ß√£o principal do Test Runner."""
    parser = argparse.ArgumentParser(description='VIBECODE Test Runner V4.0')
    parser.add_argument('--level', choices=['basic', 'enhanced', 'comprehensive'], 
                       default='basic', help='N√≠vel de testes a executar')
    parser.add_argument('--backup-protection', action='store_true',
                       help='Incluir testes de prote√ß√£o de backup')
    parser.add_argument('--memory-validation', action='store_true',
                       help='Incluir valida√ß√£o espec√≠fica de mem√≥ria')
    parser.add_argument('--project-root', type=str,
                       help='Caminho raiz do projeto')
    parser.add_argument('--report', action='store_true',
                       help='Gerar relat√≥rio detalhado')

    args = parser.parse_args()

    # Inicializar runner
    runner = TestRunner(args.project_root)

    # Executar testes baseado no n√≠vel
    if args.level == 'basic':
        results = runner.run_basic_tests(args.backup_protection)
    elif args.level == 'enhanced':
        results = runner.run_enhanced_tests(args.memory_validation)
    else:  # comprehensive
        results = runner.run_comprehensive_tests()

    # Gerar relat√≥rio se solicitado
    if args.report:
        report_file = runner.generate_report(results, args.level)
        print(f"Relat√≥rio gerado: {report_file}")

    # Determinar c√≥digo de sa√≠da
    failed_tests = [test for test, result in results.items() if not result]
    if failed_tests:
        logger.error(f"‚ùå Testes falharam: {', '.join(failed_tests)}")
        return 1
    else:
        logger.info("‚úÖ Todos os testes passaram!")
        return 0

if __name__ == "__main__":
    sys.exit(main())
