#!/usr/bin/env python3
"""
üì¶ ARCHIVE MANAGER - VIBECODE SYSTEM V4.0

Sistema inteligente de arquivamento e compacta√ß√£o para otimiza√ß√£o de espa√ßo.
Integrado com helpers VIBECODE para m√°xima efici√™ncia.

Autor: GRUPO US - VIBECODE SYSTEM
Data: 2025-01-27
"""

import sys
import shutil
import zipfile
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple

# Importar helpers VIBECODE V4.0
try:
    # Adicionar caminho dos helpers ao sys.path
    sys.path.insert(0, str(Path(__file__).parent.parent))
    from helpers.core.logger_utils import get_vibecode_logger, log_execution_start, log_execution_end, log_step
    from helpers.core.argument_parser import VibeCodeArgumentParser
    from helpers.system.path_utils import VibeCodePaths, get_directory_size, find_files_by_age
    from helpers.system.backup_utils import BackupManager
    from helpers.data.json_utils import safe_save_json, create_report_json
    HELPERS_AVAILABLE = True
except ImportError as e:
    # Fallback para compatibilidade
    import logging
    import argparse
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    logger = logging.getLogger(__name__)
    HELPERS_AVAILABLE = False
    print(f"‚ö†Ô∏è Helpers n√£o dispon√≠veis (usando fallback): {e}")

# Configurar logger
if HELPERS_AVAILABLE:
    logger = get_vibecode_logger("archive_manager")
else:
    logger = logging.getLogger(__name__)


class ArchiveManager:
    """Gerenciador inteligente de arquivamento VIBECODE V4.0."""

    def __init__(self, project_root: str = None):
        # Usar helper de paths se dispon√≠vel
        if HELPERS_AVAILABLE:
            self.paths = VibeCodePaths(project_root)
            self.project_root = self.paths.project_root
            self.project_core = self.paths.project_core
            self.archive_dir = self.paths.project_core / "archives"
            self.logs_dir = self.paths.logs_path
            self.reports_dir = self.paths.reports_path
        else:
            # Fallback para compatibilidade
            self.project_root = Path(project_root) if project_root else Path.cwd()
            self.project_core = self.project_root / "@project-core"
            self.archive_dir = self.project_core / "archives"
            self.logs_dir = self.project_core / "logs"
            self.reports_dir = self.project_core / "reports"

        # Criar diret√≥rio de arquivos se n√£o existir
        self.archive_dir.mkdir(exist_ok=True)

        # Configura√ß√µes de arquivamento
        self.config = {
            "log_retention_days": 30,
            "report_retention_days": 90,
            "max_archive_size_mb": 50,
            "compression_level": 6,
            "auto_cleanup_enabled": True,
            "external_backup_path": "C:/Users/Admin/OneDrive/Documentos/VSCODE-BACKUP",
            "project_log_retention_days": 7  # Logs no projeto apenas 7 dias
        }

        # Configurar destino externo
        self.external_backup = Path(self.config["external_backup_path"])
        if HELPERS_AVAILABLE:
            # Criar estrutura externa se n√£o existir
            try:
                self.external_backup.mkdir(parents=True, exist_ok=True)
                (self.external_backup / "logs").mkdir(exist_ok=True)
                (self.external_backup / "reports").mkdir(exist_ok=True)
                (self.external_backup / "backups").mkdir(exist_ok=True)
                (self.external_backup / "automation_backup").mkdir(exist_ok=True)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è N√£o foi poss√≠vel criar estrutura externa: {e}")
                self.external_backup = None

    def archive_old_logs(self, days_old: int = None, dry_run: bool = False) -> Dict[str, any]:
        """
        Arquiva logs antigos em arquivo compactado.

        Args:
            days_old: Idade m√≠nima dos logs (padr√£o: config)
            dry_run: Executar sem fazer altera√ß√µes

        Returns:
            Resultado do arquivamento
        """
        log_step(logger, "üì¶ Arquivando logs antigos...")

        days_old = days_old or self.config["log_retention_days"]
        cutoff_date = datetime.now() - timedelta(days=days_old)

        if not self.logs_dir.exists():
            logger.warning("‚ö†Ô∏è Diret√≥rio de logs n√£o existe")
            return {"status": "no_logs_dir", "archived_files": 0}

        # Encontrar logs antigos
        if HELPERS_AVAILABLE:
            old_logs = find_files_by_age(self.logs_dir, days_old, ["*.log", "*.txt"])
        else:
            old_logs = [f for f in self.logs_dir.rglob("*.log")
                       if datetime.fromtimestamp(f.stat().st_mtime) < cutoff_date]

        if not old_logs:
            logger.info("üì¶ Nenhum log antigo encontrado")
            return {"status": "no_old_logs", "archived_files": 0}

        if dry_run:
            logger.info(f"üîç DRY RUN: {len(old_logs)} logs seriam arquivados")
            return {"status": "dry_run", "archived_files": len(old_logs)}

        # Criar arquivo compactado
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        archive_name = f"logs_archive_{timestamp}.zip"
        archive_path = self.archive_dir / archive_name

        try:
            archived_count = 0
            with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED,
                               compresslevel=self.config["compression_level"]) as zipf:

                for log_file in old_logs:
                    # Adicionar ao arquivo
                    arcname = log_file.relative_to(self.logs_dir)
                    zipf.write(log_file, arcname)
                    archived_count += 1

                    # Remover arquivo original
                    log_file.unlink()
                    logger.info(f"üì¶ Log arquivado: {log_file.name}")

            # Criar metadados do arquivo
            metadata = create_report_json(
                title="Log Archive Report",
                data={
                    "archive_path": str(archive_path),
                    "archived_files": archived_count,
                    "cutoff_date": cutoff_date.isoformat(),
                    "archive_size_mb": archive_path.stat().st_size / (1024 * 1024)
                }
            ) if HELPERS_AVAILABLE else {
                "archive_path": str(archive_path),
                "archived_files": archived_count,
                "cutoff_date": cutoff_date.isoformat()
            }

            metadata_file = self.archive_dir / f"{archive_name}.metadata.json"
            if HELPERS_AVAILABLE:
                safe_save_json(metadata, metadata_file)

            logger.info(f"‚úÖ {archived_count} logs arquivados em {archive_name}")
            return {
                "status": "success",
                "archived_files": archived_count,
                "archive_path": str(archive_path),
                "archive_size_mb": round(archive_path.stat().st_size / (1024 * 1024), 2)
            }

        except Exception as e:
            logger.error(f"‚ùå Erro ao arquivar logs: {e}")
            return {"status": "error", "error": str(e), "archived_files": 0}

    def archive_old_reports(self, days_old: int = None, dry_run: bool = False) -> Dict[str, any]:
        """
        Arquiva relat√≥rios antigos em arquivo compactado.

        Args:
            days_old: Idade m√≠nima dos relat√≥rios (padr√£o: config)
            dry_run: Executar sem fazer altera√ß√µes

        Returns:
            Resultado do arquivamento
        """
        log_step(logger, "üìä Arquivando relat√≥rios antigos...")

        days_old = days_old or self.config["report_retention_days"]
        cutoff_date = datetime.now() - timedelta(days=days_old)

        if not self.reports_dir.exists():
            logger.warning("‚ö†Ô∏è Diret√≥rio de relat√≥rios n√£o existe")
            return {"status": "no_reports_dir", "archived_files": 0}

        # Encontrar relat√≥rios antigos
        if HELPERS_AVAILABLE:
            old_reports = find_files_by_age(self.reports_dir, days_old, ["*.json", "*.md", "*.txt"])
        else:
            old_reports = [f for f in self.reports_dir.rglob("*")
                          if f.is_file() and datetime.fromtimestamp(f.stat().st_mtime) < cutoff_date]

        if not old_reports:
            logger.info("üìä Nenhum relat√≥rio antigo encontrado")
            return {"status": "no_old_reports", "archived_files": 0}

        if dry_run:
            logger.info(f"üîç DRY RUN: {len(old_reports)} relat√≥rios seriam arquivados")
            return {"status": "dry_run", "archived_files": len(old_reports)}

        # Criar arquivo compactado
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        archive_name = f"reports_archive_{timestamp}.zip"
        archive_path = self.archive_dir / archive_name

        try:
            archived_count = 0
            with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED,
                               compresslevel=self.config["compression_level"]) as zipf:

                for report_file in old_reports:
                    # Adicionar ao arquivo
                    arcname = report_file.relative_to(self.reports_dir)
                    zipf.write(report_file, arcname)
                    archived_count += 1

                    # Remover arquivo original
                    report_file.unlink()
                    logger.info(f"üìä Relat√≥rio arquivado: {report_file.name}")

            logger.info(f"‚úÖ {archived_count} relat√≥rios arquivados em {archive_name}")
            return {
                "status": "success",
                "archived_files": archived_count,
                "archive_path": str(archive_path),
                "archive_size_mb": round(archive_path.stat().st_size / (1024 * 1024), 2)
            }

        except Exception as e:
            logger.error(f"‚ùå Erro ao arquivar relat√≥rios: {e}")
            return {"status": "error", "error": str(e), "archived_files": 0}

    def cleanup_old_archives(self, days_old: int = 180, dry_run: bool = False) -> Dict[str, any]:
        """
        Remove arquivos antigos para economizar espa√ßo.

        Args:
            days_old: Idade m√≠nima dos arquivos (padr√£o: 180 dias)
            dry_run: Executar sem fazer altera√ß√µes

        Returns:
            Resultado da limpeza
        """
        log_step(logger, "üßπ Limpando arquivos antigos...")

        cutoff_date = datetime.now() - timedelta(days=days_old)

        if not self.archive_dir.exists():
            logger.warning("‚ö†Ô∏è Diret√≥rio de arquivos n√£o existe")
            return {"status": "no_archive_dir", "cleaned_files": 0}

        # Encontrar arquivos antigos
        old_archives = [f for f in self.archive_dir.rglob("*")
                       if f.is_file() and datetime.fromtimestamp(f.stat().st_mtime) < cutoff_date]

        if not old_archives:
            logger.info("üßπ Nenhum arquivo antigo encontrado")
            return {"status": "no_old_archives", "cleaned_files": 0}

        if dry_run:
            logger.info(f"üîç DRY RUN: {len(old_archives)} arquivos seriam removidos")
            return {"status": "dry_run", "cleaned_files": len(old_archives)}

        # Remover arquivos antigos
        cleaned_count = 0
        total_size_freed = 0

        for archive_file in old_archives:
            try:
                file_size = archive_file.stat().st_size
                archive_file.unlink()
                cleaned_count += 1
                total_size_freed += file_size
                logger.info(f"üóëÔ∏è Arquivo removido: {archive_file.name}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erro ao remover {archive_file.name}: {e}")

        logger.info(f"‚úÖ {cleaned_count} arquivos removidos, {total_size_freed / (1024*1024):.2f}MB liberados")
        return {
            "status": "success",
            "cleaned_files": cleaned_count,
            "size_freed_mb": round(total_size_freed / (1024 * 1024), 2)
        }

    def get_archive_stats(self) -> Dict[str, any]:
        """
        Obt√©m estat√≠sticas dos arquivos.

        Returns:
            Estat√≠sticas dos arquivos
        """
        log_step(logger, "üìä Coletando estat√≠sticas de arquivos...")

        if not self.archive_dir.exists():
            return {"total_archives": 0, "total_size_mb": 0}

        archives = list(self.archive_dir.rglob("*.zip"))
        total_size = sum(f.stat().st_size for f in archives)

        stats = {
            "total_archives": len(archives),
            "total_size_mb": round(total_size / (1024 * 1024), 2),
            "archive_dir": str(self.archive_dir),
            "oldest_archive": None,
            "newest_archive": None
        }

        if archives:
            oldest = min(archives, key=lambda x: x.stat().st_mtime)
            newest = max(archives, key=lambda x: x.stat().st_mtime)
            stats["oldest_archive"] = oldest.name
            stats["newest_archive"] = newest.name

        logger.info(f"üìä {stats['total_archives']} arquivos, {stats['total_size_mb']}MB total")
        return stats

    def validate_file_separation_policy(self) -> Dict[str, any]:
        """
        Valida a pol√≠tica de separa√ß√£o de arquivos.

        Returns:
            Resultado da valida√ß√£o
        """
        log_step(logger, "üîç Validando pol√≠tica de separa√ß√£o de arquivos...")

        violations = []
        warnings = []

        # Verificar logs antigos no projeto
        if self.logs_dir.exists():
            cutoff_date = datetime.now() - timedelta(days=self.config["project_log_retention_days"])
            old_logs = [f for f in self.logs_dir.rglob("*.log")
                       if datetime.fromtimestamp(f.stat().st_mtime) < cutoff_date]

            if old_logs:
                violations.append(f"{len(old_logs)} logs antigos encontrados no projeto (>{self.config['project_log_retention_days']} dias)")

        # Verificar relat√≥rios tempor√°rios
        if self.reports_dir.exists():
            temp_reports = list(self.reports_dir.glob("backup-*.json"))
            temp_reports.extend(self.reports_dir.glob("smart-backup-*.json"))
            temp_reports.extend(self.reports_dir.glob("test_report_*.json"))

            if len(temp_reports) > 1:  # Permitir apenas 1 relat√≥rio de teste recente
                violations.append(f"{len(temp_reports)} relat√≥rios tempor√°rios encontrados no projeto")

        # Verificar tamanho total do projeto
        if HELPERS_AVAILABLE:
            total_size, _ = get_directory_size(self.project_root)
            size_mb = total_size / (1024 * 1024)

            if size_mb > 50:  # Limite de 50MB para GitHub
                warnings.append(f"Projeto muito grande: {size_mb:.1f}MB > 50MB")

        # Verificar se estrutura externa existe
        if self.external_backup and not self.external_backup.exists():
            warnings.append("Estrutura de backup externa n√£o encontrada")

        status = "compliant" if not violations else "violations_found"
        if warnings and status == "compliant":
            status = "warnings_found"

        result = {
            "status": status,
            "violations": violations,
            "warnings": warnings,
            "project_size_mb": size_mb if HELPERS_AVAILABLE else 0,
            "external_backup_available": self.external_backup.exists() if self.external_backup else False
        }

        if violations:
            logger.warning(f"‚ö†Ô∏è {len(violations)} viola√ß√µes da pol√≠tica encontradas")
        elif warnings:
            logger.info(f"‚ÑπÔ∏è {len(warnings)} avisos encontrados")
        else:
            logger.info("‚úÖ Pol√≠tica de separa√ß√£o de arquivos em compliance")

        return result

    def cleanup_project_for_github(self, dry_run: bool = False) -> Dict[str, any]:
        """
        Limpa projeto para otimizar para GitHub.

        Args:
            dry_run: Executar sem fazer altera√ß√µes

        Returns:
            Resultado da limpeza
        """
        log_step(logger, "üßπ Limpando projeto para GitHub...")

        if dry_run:
            logger.info("üîç DRY RUN: Simulando limpeza...")

        moved_files = []

        # Mover logs antigos para backup externo
        if self.logs_dir.exists() and self.external_backup:
            cutoff_date = datetime.now() - timedelta(days=self.config["project_log_retention_days"])
            old_logs = [f for f in self.logs_dir.rglob("*.log")
                       if datetime.fromtimestamp(f.stat().st_mtime) < cutoff_date]

            for log_file in old_logs:
                if not dry_run and self.external_backup:
                    dest = self.external_backup / "logs" / log_file.name
                    try:
                        shutil.move(str(log_file), str(dest))
                        moved_files.append(f"log: {log_file.name}")
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Erro ao mover {log_file.name}: {e}")
                else:
                    moved_files.append(f"log: {log_file.name} (dry-run)")

        # Mover relat√≥rios tempor√°rios
        if self.reports_dir.exists() and self.external_backup:
            temp_reports = list(self.reports_dir.glob("backup-*.json"))
            temp_reports.extend(self.reports_dir.glob("smart-backup-*.json"))

            # Manter apenas o relat√≥rio de teste mais recente
            test_reports = list(self.reports_dir.glob("test_report_*.json"))
            if len(test_reports) > 1:
                test_reports.sort(key=lambda x: x.stat().st_mtime, reverse=True)
                temp_reports.extend(test_reports[1:])  # Mover todos exceto o mais recente

            for report in temp_reports:
                if not dry_run and self.external_backup:
                    dest = self.external_backup / "reports" / report.name
                    try:
                        shutil.move(str(report), str(dest))
                        moved_files.append(f"report: {report.name}")
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Erro ao mover {report.name}: {e}")
                else:
                    moved_files.append(f"report: {report.name} (dry-run)")

        result = {
            "status": "success" if not dry_run else "dry_run",
            "moved_files": len(moved_files),
            "files_list": moved_files,
            "external_backup_used": str(self.external_backup) if self.external_backup else None
        }

        if dry_run:
            logger.info(f"üîç DRY RUN: {len(moved_files)} arquivos seriam movidos")
        else:
            logger.info(f"‚úÖ {len(moved_files)} arquivos movidos para backup externo")

        return result


def main():
    """Fun√ß√£o principal do Archive Manager."""
    # Usar helper de argument parser se dispon√≠vel
    if HELPERS_AVAILABLE:
        parser = VibeCodeArgumentParser("archive_manager", 'VIBECODE Archive Manager V4.0')

        # Adicionar argumentos espec√≠ficos de arquivamento
        parser.add_custom_argument('--archive-logs', action='store_true', help='Arquivar logs antigos')
        parser.add_custom_argument('--archive-reports', action='store_true', help='Arquivar relat√≥rios antigos')
        parser.add_custom_argument('--cleanup-archives', action='store_true', help='Limpar arquivos antigos')
        parser.add_custom_argument('--stats', action='store_true', help='Mostrar estat√≠sticas')
        parser.add_custom_argument('--validate-policy', action='store_true', help='Validar pol√≠tica de separa√ß√£o')
        parser.add_custom_argument('--cleanup-for-github', action='store_true', help='Limpar projeto para GitHub')
        parser.add_custom_argument('--days-old', type=int, help='Idade m√≠nima em dias')
        parser.add_custom_argument('--project-root', type=str, help='Caminho raiz do projeto')

        args = parser.parse_args()

        # Log de in√≠cio de execu√ß√£o
        log_execution_start(logger, "archive_manager", vars(args))
    else:
        # Fallback para parser manual
        import argparse
        parser = argparse.ArgumentParser(description='VIBECODE Archive Manager V4.0')
        parser.add_argument('--archive-logs', action='store_true', help='Arquivar logs antigos')
        parser.add_argument('--archive-reports', action='store_true', help='Arquivar relat√≥rios antigos')
        parser.add_argument('--cleanup-archives', action='store_true', help='Limpar arquivos antigos')
        parser.add_argument('--stats', action='store_true', help='Mostrar estat√≠sticas')
        parser.add_argument('--dry-run', action='store_true', help='Executar sem fazer altera√ß√µes')
        parser.add_argument('--days-old', type=int, help='Idade m√≠nima em dias')
        parser.add_argument('--project-root', type=str, help='Caminho raiz do projeto')
        args = parser.parse_args()

    start_time = datetime.now()

    # Inicializar manager
    manager = ArchiveManager(args.project_root)

    success = True

    # Executar a√ß√µes solicitadas
    if args.archive_logs:
        result = manager.archive_old_logs(args.days_old, args.dry_run)
        print(f"üì¶ Logs: {result['status']} - {result['archived_files']} arquivos")
        success &= result['status'] in ['success', 'no_old_logs', 'dry_run']

    if args.archive_reports:
        result = manager.archive_old_reports(args.days_old, args.dry_run)
        print(f"üìä Relat√≥rios: {result['status']} - {result['archived_files']} arquivos")
        success &= result['status'] in ['success', 'no_old_reports', 'dry_run']

    if args.cleanup_archives:
        result = manager.cleanup_old_archives(args.days_old or 180, args.dry_run)
        print(f"üßπ Limpeza: {result['status']} - {result['cleaned_files']} arquivos")
        success &= result['status'] in ['success', 'no_old_archives', 'dry_run']

    if args.stats:
        stats = manager.get_archive_stats()
        print(f"üìä Estat√≠sticas: {stats['total_archives']} arquivos, {stats['total_size_mb']}MB")

    if args.validate_policy:
        result = manager.validate_file_separation_policy()
        print(f"üìã Status: {result['status']}")
        if result['violations']:
            print("‚ùå Viola√ß√µes:")
            for violation in result['violations']:
                print(f"  - {violation}")
        if result['warnings']:
            print("‚ö†Ô∏è Avisos:")
            for warning in result['warnings']:
                print(f"  - {warning}")

    if args.cleanup_for_github:
        result = manager.cleanup_project_for_github(dry_run=args.dry_run)
        print(f"üßπ Limpeza: {result['moved_files']} arquivos processados")
        if result['external_backup_used']:
            print(f"üìÅ Backup externo: {result['external_backup_used']}")

    # Se nenhuma a√ß√£o especificada, mostrar help
    if not any([args.archive_logs, args.archive_reports, args.cleanup_archives, args.stats, args.validate_policy, args.cleanup_for_github]):
        parser.print_help()
        return 0

    # Log de fim de execu√ß√£o
    if HELPERS_AVAILABLE:
        duration = (datetime.now() - start_time).total_seconds()
        log_execution_end(logger, "archive_manager", success, duration)

    return 0 if success else 1


if __name__ == "__main__":
    import sys
    sys.exit(main())
